{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of copy8E&F_LR_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HExLQrE4ZxR"
      },
      "source": [
        "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LuKrFzC4ZxV"
      },
      "source": [
        "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wES-wWN4ZxX"
      },
      "source": [
        "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
        "\n",
        "Check the documentation for better understanding of these attributes: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
        "\n",
        "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
        "\n",
        "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
        "\n",
        "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
        "\n",
        "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
        "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
        "\n",
        "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
        "\n",
        "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z830CfMk4Zxa"
      },
      "source": [
        "## Task E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuBxHiCQ4Zxc"
      },
      "source": [
        "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
        "\n",
        "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
        "\n",
        "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCgMNEvI4Zxf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUNIqCe4Zxn"
      },
      "source": [
        "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywdzvTdxGe0I"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x, x_test, y, y_test = train_test_split(X, y,test_size=0.2,train_size=0.8)\n",
        "x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlmeKQ5zG4NH",
        "outputId": "94c3156a-8bf2-4d6c-93f1-fb59dfd53d64"
      },
      "source": [
        "x_train.shape, x_test.shape,x_cv.shape\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3000, 5), (1000, 5), (1000, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHie1zqH4Zxt"
      },
      "source": [
        "### Pseudo code\n",
        "\n",
        "clf = SVC(gamma=0.001, C=100.)<br>\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
        "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
        "    \n",
        "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
        "\n",
        "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h43kDT3M41u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3271b8-ad62-4e43-9284-c10cfc066007"
      },
      "source": [
        "# you can write your code here\n",
        "clf = SVC(gamma=0.001, C=100)\n",
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nIWfpn-2RnE",
        "outputId": "dc3e8cd7-5d44-413c-9386-d930c34f2f87"
      },
      "source": [
        "clf.get_params, clf.kernel, clf.intercept_"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<bound method BaseEstimator.get_params of SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "     decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
              "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "     tol=0.001, verbose=False)>, 'rbf', array([0.77561337]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mXOvrlrfP7p"
      },
      "source": [
        "# Get parameters from model\n",
        "params = clf.get_params()\n",
        "sv = clf.support_vectors_\n",
        "nv = clf.n_support_\n",
        "a  = clf.dual_coef_ # this is alpha only for support vectors, for nonsupport vectors alpha = 0\n",
        "b  = clf.intercept_\n",
        "cs = clf.classes_\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPmJjJ2VpzQH"
      },
      "source": [
        "def decision_function(x_cv):\n",
        "  final_val = []\n",
        "  for xq in x_cv:\n",
        "    sum = 0\n",
        "    for i in range(len(sv)):   \n",
        "      \n",
        "      l2_norm = np.linalg.norm(xq - sv[i])\n",
        "      pow = np.exp(-params['gamma'] * (l2_norm ** 2))\n",
        "      sum +=  a[0][i] * pow  \n",
        "    v = sum + clf.intercept_\n",
        "    final_val.append(v[0])\n",
        "  return final_val"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxhjyx4PCAWN"
      },
      "source": [
        "impl_decision = decision_function(x_cv)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB8pMUnVbk5w",
        "outputId": "0e803e57-9f06-44ca-c912-523429a2fd28"
      },
      "source": [
        "impl_decision[:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5649349687115753,\n",
              " -2.860953842059111,\n",
              " 1.3385991865591573,\n",
              " -0.7213387141661306,\n",
              " -1.8891645399903116]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DED6CvSACDe0",
        "outputId": "9a995638-aaeb-4d74-e3c2-0d88a9008c3a"
      },
      "source": [
        "sklearn_p = clf.decision_function(x_cv)\n",
        "print (sklearn_p[:5])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.56493497 -2.86095384  1.33859919 -0.72133871 -1.88916454]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6NP7n0roFyF"
      },
      "source": [
        "fcv = final_val"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bKCboN4Zxu"
      },
      "source": [
        "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMn7OEN94Zxw"
      },
      "source": [
        "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
        "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0n5EFkx4Zxz"
      },
      "source": [
        "## TASK F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HOqVJq4Zx1"
      },
      "source": [
        "\n",
        "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
        "\n",
        "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
        "\n",
        "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
        "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
        "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
        "\n",
        "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTY7z2bd4Zx2"
      },
      "source": [
        "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM3odN1Z4Zx3"
      },
      "source": [
        "\n",
        "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
        "\n",
        "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
        "\n",
        "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
        "\n",
        "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
        "\n",
        "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBpcQZuAZLHt"
      },
      "source": [
        "def initialize_weights(dim):\n",
        "  ''' In this function, we will initialize our weights and bias''' \n",
        "  #initialize the weights to zeros array of (1,dim) dimensions\n",
        "  #you use zeros_like function to initialize zero, check this link https:// #initialize bias to zero\n",
        "  w = np.zeros_like(dim)\n",
        "  b=0\n",
        "  return w,b \n",
        "\n",
        "def sigmoid(z):\n",
        "  ''' In this function, we will return sigmoid of z'''\n",
        "  # compute sigmoid(z) and return\n",
        "  sig_z=1/(1+(np.exp(-z )))\n",
        "  return sig_z\n",
        "\n",
        "\n",
        "def logloss(y_true,y_pred):\n",
        "  '''In this function, we will compute log loss '''\n",
        "  n = len(y_true) \n",
        "  s=0\n",
        "  for i in range(n):\n",
        "    t = y_true[i]*np.log10(y_pred[i])+ (1.0-y_true[i])*np.log10(1.0-y_pred[i])\n",
        "    s=s+t\n",
        "  loss = ((-1.0) / n) * s \n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "  '''In this function, we will compute the gardient w.r.to w '''\n",
        "  dw =x*(y-sigmoid(np.dot(w,x)+b)) - ((alpha*w)/N)\n",
        "  return dw\n",
        "\n",
        "\n",
        "def gradient_db(x,y,w,b):\n",
        "  '''In this function, we will compute gradient w.r.to b '''\n",
        "  db = y-sigmoid(np.dot(w,x)+b) \n",
        "  return db"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxMh0_iIaBpj",
        "outputId": "ec3bc7ec-b814-42ce-8b16-62e830db2f0e"
      },
      "source": [
        "def plat_scaling(y_train , y_cv): \n",
        "  y_cv_plat= []\n",
        "  plus= ( np.count_nonzero(y_train==1)) \n",
        "  minus= ( np.count_nonzero(y_train==0))\n",
        "  y_plus= (plus+1)/(plus+2) \n",
        "  y_minus=1/(minus-2)\n",
        "  for i in range(len(y_cv)):\n",
        "    if y_cv[i] == 1: \n",
        "      y_cv_plat.append(y_plus)\n",
        "    if y_cv[i] == 0: \n",
        "      y_cv_plat.append(y_minus)\n",
        "  return np.array(y_cv_plat) \n",
        "\n",
        "y_cv_plat=plat_scaling(y_train,y_cv) \n",
        "print(y_cv_plat[:6])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.98878924e-01 4.74383302e-04 9.98878924e-01 4.74383302e-04\n",
            " 4.74383302e-04 9.98878924e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zArFdly1aBId"
      },
      "source": [
        "def train(X_train,y_train,epochs,alpha,eta0):\n",
        "  ''' In this function, we will implement logistic regression'''\n",
        "  #Here eta0 is learning rate\n",
        "  #implement the code as follows\n",
        "  # initalize the weights call the initialize_weights(X_train[0] function \n",
        "  w,b = initialize_weights(X_train[0])\n",
        "  train_loss = []\n",
        "  test_loss = []\n",
        "  # for every epoch\n",
        "  for epoch in range(0,epochs):\n",
        "    # for every data point(X_train,y_train)\n",
        "    ypred_train = []\n",
        "    ypred_test = []\n",
        "    for x,y in zip(X_train,y_train):\n",
        "      #compute gradient w.r.to w (call the gradient_dw() function)\n",
        "      dw = gradient_dw(x,y,w,b,alpha,len(X_train))\n",
        "      #compute gradient w.r.to b (call the gradient_db() function) \n",
        "      db = gradient_db(x,y,w,b)\n",
        "      #update w, b\n",
        "      w += eta0*dw\n",
        "      b += eta0*db\n",
        "\n",
        "    # predict the output of x_train[for all data points in X_train] using\n",
        "    for x in X_train: \n",
        "      ypred_train.append(sigmoid(np.dot(w,x) + b))\n",
        "    \n",
        "    #compute the loss between predicted and actual values \n",
        "    tr_loss = logloss(y_train,ypred_train)\n",
        "\n",
        "    # append all the train loss values in a list \n",
        "    train_loss.append(logloss(y_train,ypred_train))\n",
        "    # predict the output of x_test[for all data points in X_test] using w \n",
        "    \n",
        "    for x in x_test:\n",
        "      ypred_test.append(sigmoid(np.dot(w,x) + b))\n",
        "    \n",
        "    #compute the loss between predicted and actual values  \n",
        "    te_loss = logloss(y_test, ypred_test)\n",
        "    # store all the test loss values in a list \n",
        "    test_loss.append(logloss(y_test,ypred_test)) \n",
        "    \n",
        "  return w, b, train_loss"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y9OI9b2aA8b",
        "outputId": "69a92388-be6a-47e4-a271-dcd0c4944b68"
      },
      "source": [
        "alpha=0.0001\n",
        "eta0=0.0001\n",
        "N=len(x_train)\n",
        "epochs= 50 \n",
        "w,b,train_loss=train(impl_decision,y_cv_plat,epochs,alpha,eta0) \n",
        "print(w)\n",
        "print(b) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2056950102701298\n",
            "-0.10561670490574834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vnx7PzuScW6I",
        "outputId": "e50fc43f-1501-4d00-aeee-9095b2e911cf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "e = [i for i in range(1, 51)]\n",
        "plt.plot(e,train_loss,label = 'Train_loss') \n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title('Epoch Number VS Train Loss') \n",
        "plt.xlabel('Epoch Number') \n",
        "plt.ylabel('Train Loss')\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5r5ipBdakn"
      },
      "source": [
        "f_test=clf.decision_function(x_test)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E8GA57AdgIH",
        "outputId": "b3cab984-add7-4e21-e17b-06fe05f67015"
      },
      "source": [
        "prob_list = [] \n",
        "for x_q in f_test:\n",
        "  temp=1/(1+np.exp(-w*x_q-b)) \n",
        "  prob_list.append(temp)\n",
        "prob_list"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8899518103197805,\n",
              " 0.12511692833084803,\n",
              " 0.04649608899223197,\n",
              " 0.9103530958600399,\n",
              " 0.8600762544048548,\n",
              " 0.7069232427013736,\n",
              " 0.8644617642453217,\n",
              " 0.6663083249618144,\n",
              " 0.0029578003984514225,\n",
              " 0.03798799330517371,\n",
              " 0.03987801695206759,\n",
              " 0.05481218803160806,\n",
              " 0.019524664286825134,\n",
              " 0.07751758953999711,\n",
              " 0.018108992324817472,\n",
              " 0.752800876512436,\n",
              " 0.3989686813974159,\n",
              " 0.40135229807312833,\n",
              " 0.11076071148991656,\n",
              " 0.05101044867089022,\n",
              " 0.03786123580076236,\n",
              " 0.39555963055430626,\n",
              " 0.9182141888681041,\n",
              " 0.015643697500998806,\n",
              " 0.7523349575381165,\n",
              " 0.09521675252047121,\n",
              " 0.9218412227717747,\n",
              " 0.022868741093174157,\n",
              " 0.00936171621057745,\n",
              " 0.049820929494310016,\n",
              " 0.07293625506189785,\n",
              " 0.8793026323363301,\n",
              " 0.04712634284753917,\n",
              " 0.24612356425938525,\n",
              " 0.01924802823583422,\n",
              " 0.055279399613824706,\n",
              " 0.514341587294535,\n",
              " 0.2532816610169808,\n",
              " 0.9535517186567449,\n",
              " 0.13178597157521457,\n",
              " 0.10888021012913937,\n",
              " 0.7992765699934358,\n",
              " 0.015753829638835867,\n",
              " 0.9053074666731139,\n",
              " 0.9109411321157795,\n",
              " 0.9441996611844822,\n",
              " 0.013650154022713678,\n",
              " 0.39185935341501,\n",
              " 0.006882100525506797,\n",
              " 0.06990833072330838,\n",
              " 0.018709361220095016,\n",
              " 0.8790365042476189,\n",
              " 0.009262514438896719,\n",
              " 0.014686283549293556,\n",
              " 0.8599902911343591,\n",
              " 0.31208322058662763,\n",
              " 0.11694677772384832,\n",
              " 0.011368937672176416,\n",
              " 0.11923289373538222,\n",
              " 0.31507300257709153,\n",
              " 0.0029214801526115356,\n",
              " 0.07623618342951834,\n",
              " 0.1573092656029013,\n",
              " 0.9380207609112129,\n",
              " 0.1564394091181698,\n",
              " 0.05275461988523734,\n",
              " 0.041067023249462764,\n",
              " 0.90882284080408,\n",
              " 0.03457473679317557,\n",
              " 0.01662397138250302,\n",
              " 0.02509327177667671,\n",
              " 0.7632730194349703,\n",
              " 0.9725455569936785,\n",
              " 0.20733086913554027,\n",
              " 0.03205523959140735,\n",
              " 0.03143970268819645,\n",
              " 0.06546039665275112,\n",
              " 0.9548065954690019,\n",
              " 0.9243647255520752,\n",
              " 0.06471318604581221,\n",
              " 0.7287015366139972,\n",
              " 0.00685129992235902,\n",
              " 0.012789970149699851,\n",
              " 0.014466717964136998,\n",
              " 0.05862758876221762,\n",
              " 0.07388376374991593,\n",
              " 0.7605448444931067,\n",
              " 0.8698017497516718,\n",
              " 0.6541134449517317,\n",
              " 0.8569620607456547,\n",
              " 0.848056310427031,\n",
              " 0.07385773374579509,\n",
              " 0.034510441140771855,\n",
              " 0.10020161081829386,\n",
              " 0.00781054372640051,\n",
              " 0.37514362018658165,\n",
              " 0.08291681957775966,\n",
              " 0.6004898962673988,\n",
              " 0.05333148811698977,\n",
              " 0.04228417470636401,\n",
              " 0.8818642910947934,\n",
              " 0.020541449570313675,\n",
              " 0.5197562637425797,\n",
              " 0.0025379183778444993,\n",
              " 0.20653423846082314,\n",
              " 0.047647809325271684,\n",
              " 0.0053458253414863524,\n",
              " 0.027982376824522266,\n",
              " 0.042124428578720154,\n",
              " 0.016334053335589564,\n",
              " 0.11373399434855015,\n",
              " 0.022805290910536357,\n",
              " 0.03712652679574515,\n",
              " 0.026996844904859436,\n",
              " 0.02739423788490827,\n",
              " 0.8963121045096528,\n",
              " 0.028348862197474456,\n",
              " 0.8086069362955484,\n",
              " 0.01825213925055243,\n",
              " 0.8731772493543651,\n",
              " 0.8126023451760085,\n",
              " 0.06440397456283183,\n",
              " 0.8760158980789058,\n",
              " 0.08705617432888273,\n",
              " 0.004967672849722434,\n",
              " 0.011724657447498009,\n",
              " 0.7274607991997474,\n",
              " 0.05036875173266611,\n",
              " 0.03639077549819683,\n",
              " 0.6268333722147131,\n",
              " 0.06173424855223352,\n",
              " 0.7610119040583413,\n",
              " 0.026129955241007224,\n",
              " 0.12058161848373798,\n",
              " 0.014622768285666091,\n",
              " 0.9301223167531295,\n",
              " 0.026941799572806402,\n",
              " 0.9916255232382835,\n",
              " 0.019830337016370676,\n",
              " 0.9065842819379026,\n",
              " 0.03304898614588552,\n",
              " 0.2180521687133001,\n",
              " 0.6924604703749498,\n",
              " 0.0885518061696195,\n",
              " 0.1009600361617826,\n",
              " 0.05103854609627054,\n",
              " 0.05003487753552743,\n",
              " 0.0159502086076122,\n",
              " 0.004580942884717327,\n",
              " 0.014828569713338123,\n",
              " 0.008162054616555947,\n",
              " 0.004615417768694588,\n",
              " 0.02958396798072902,\n",
              " 0.5549185846702942,\n",
              " 0.05753148832911608,\n",
              " 0.0432598490114881,\n",
              " 0.689783359686805,\n",
              " 0.819020951887292,\n",
              " 0.8978104736848189,\n",
              " 0.6839072654477385,\n",
              " 0.024708072627372877,\n",
              " 0.043199596598675165,\n",
              " 0.8785432042114374,\n",
              " 0.011274554902589955,\n",
              " 0.020036382198060327,\n",
              " 0.38485149903693583,\n",
              " 0.22908150257640567,\n",
              " 0.07157506507580873,\n",
              " 0.05865405319553366,\n",
              " 0.040312770216227964,\n",
              " 0.02197135867396602,\n",
              " 0.1072195552648009,\n",
              " 0.9361027982378264,\n",
              " 0.11838423072042067,\n",
              " 0.033535032263080876,\n",
              " 0.14324864120648229,\n",
              " 0.03145646206252989,\n",
              " 0.02646403092123458,\n",
              " 0.015518432738808475,\n",
              " 0.0616346388800727,\n",
              " 0.32387479458992247,\n",
              " 0.8675955286946441,\n",
              " 0.13578306416664113,\n",
              " 0.023515449819370513,\n",
              " 0.04590804518391321,\n",
              " 0.8851092833953348,\n",
              " 0.014279295036843064,\n",
              " 0.02780723284407571,\n",
              " 0.009765354577781866,\n",
              " 0.011522274474409783,\n",
              " 0.06327436740095047,\n",
              " 0.06829814208806807,\n",
              " 0.7745089396847256,\n",
              " 0.04257574255201825,\n",
              " 0.7009956924223278,\n",
              " 0.8909040320323536,\n",
              " 0.012829921905996371,\n",
              " 0.14018257324301472,\n",
              " 0.750832136785816,\n",
              " 0.7643643637772889,\n",
              " 0.040716428786111855,\n",
              " 0.03295941606408416,\n",
              " 0.6208858548449925,\n",
              " 0.007282337852165574,\n",
              " 0.3023237940608941,\n",
              " 0.006020978427858258,\n",
              " 0.04475874531653667,\n",
              " 0.01739142352000477,\n",
              " 0.015739028011025193,\n",
              " 0.6664172765710056,\n",
              " 0.08418113907463584,\n",
              " 0.028247344289657413,\n",
              " 0.6463183758265066,\n",
              " 0.1472900720677055,\n",
              " 0.18844107869781807,\n",
              " 0.2999840912325559,\n",
              " 0.0074240554451323534,\n",
              " 0.9125305648824197,\n",
              " 0.26744003971051084,\n",
              " 0.01623725818907529,\n",
              " 0.8342180128353723,\n",
              " 0.09279054645545634,\n",
              " 0.11954359538910667,\n",
              " 0.16623083502736577,\n",
              " 0.011964998013750784,\n",
              " 0.3907409459859531,\n",
              " 0.06521817012746252,\n",
              " 0.00860244067170618,\n",
              " 0.02958939935397204,\n",
              " 0.023486231533515974,\n",
              " 0.012425739552298804,\n",
              " 0.7721144752897806,\n",
              " 0.8793046528165379,\n",
              " 0.03504446024730276,\n",
              " 0.0036907776476723954,\n",
              " 0.13167604974442929,\n",
              " 0.08349637456000716,\n",
              " 0.08949721659245417,\n",
              " 0.0033129768799032492,\n",
              " 0.4719443533133075,\n",
              " 0.8922612913033274,\n",
              " 0.026221934633739445,\n",
              " 0.31584741149031154,\n",
              " 0.12367159001515053,\n",
              " 0.06993304675982202,\n",
              " 0.9262232484242564,\n",
              " 0.007684830681966603,\n",
              " 0.017109202334001254,\n",
              " 0.016383922660682927,\n",
              " 0.5084756782454871,\n",
              " 0.09269520718409875,\n",
              " 0.3533357996023024,\n",
              " 0.010658729503705156,\n",
              " 0.885281071482185,\n",
              " 0.09447184646906917,\n",
              " 0.8713298114857436,\n",
              " 0.8982291280487882,\n",
              " 0.04657543544206385,\n",
              " 0.16323252959113993,\n",
              " 0.16024659778879186,\n",
              " 0.429085941153556,\n",
              " 0.015323149511073178,\n",
              " 0.5531983072906208,\n",
              " 0.015747110291352964,\n",
              " 0.8577188480364624,\n",
              " 0.01451937356778456,\n",
              " 0.004021419253004634,\n",
              " 0.6122103621678765,\n",
              " 0.3395915840293578,\n",
              " 0.041058331889611804,\n",
              " 0.034831349580412306,\n",
              " 0.052200636938011094,\n",
              " 0.8723186992536096,\n",
              " 0.16095236499979076,\n",
              " 0.2544016071865831,\n",
              " 0.2947916534689515,\n",
              " 0.04780979760691187,\n",
              " 0.02589269130358389,\n",
              " 0.02436240035480048,\n",
              " 0.12570792732769123,\n",
              " 0.07483963888742406,\n",
              " 0.05290476253793755,\n",
              " 0.05357545198632771,\n",
              " 0.3130569080481866,\n",
              " 0.04304075577955691,\n",
              " 0.5858166419703968,\n",
              " 0.04306186059004621,\n",
              " 0.03245252562568439,\n",
              " 0.03843652662373487,\n",
              " 0.521048571212913,\n",
              " 0.008070756738116296,\n",
              " 0.02259274347744742,\n",
              " 0.909916871521705,\n",
              " 0.1498684960289375,\n",
              " 0.3562873418647604,\n",
              " 0.1697784623503601,\n",
              " 0.038660037609870426,\n",
              " 0.027633912548855444,\n",
              " 0.033565595202481605,\n",
              " 0.06452834556614002,\n",
              " 0.07437218986814938,\n",
              " 0.0244575569390001,\n",
              " 0.7750859492442664,\n",
              " 0.8274802017658831,\n",
              " 0.7672350029362294,\n",
              " 0.17526539196367788,\n",
              " 0.9240185419287171,\n",
              " 0.021082036845252023,\n",
              " 0.10608538050442215,\n",
              " 0.014238397972849776,\n",
              " 0.016928829040072986,\n",
              " 0.03171798207176149,\n",
              " 0.4231293672337758,\n",
              " 0.04337175102833227,\n",
              " 0.05934685041620337,\n",
              " 0.0678376552015614,\n",
              " 0.02263781177028328,\n",
              " 0.07089182529648268,\n",
              " 0.9123060006959532,\n",
              " 0.6357654983913291,\n",
              " 0.12233026401188367,\n",
              " 0.05516494126602242,\n",
              " 0.8384657251703101,\n",
              " 0.015807130384545006,\n",
              " 0.13146541709750142,\n",
              " 0.045355629881158266,\n",
              " 0.17555427293430037,\n",
              " 0.008835894155431009,\n",
              " 0.12788947672617298,\n",
              " 0.9435925539311982,\n",
              " 0.13139382169459699,\n",
              " 0.9524276508886625,\n",
              " 0.04057120639449423,\n",
              " 0.11869798248335958,\n",
              " 0.025123840807267143,\n",
              " 0.08066241951258027,\n",
              " 0.011786740052890097,\n",
              " 0.1481540109469944,\n",
              " 0.04238970887844144,\n",
              " 0.05388023198227072,\n",
              " 0.10261548122182482,\n",
              " 0.03530418637532963,\n",
              " 0.10290730490462417,\n",
              " 0.9443337266439522,\n",
              " 0.020670632736246364,\n",
              " 0.6547893176197843,\n",
              " 0.02144393246333866,\n",
              " 0.016328785232430216,\n",
              " 0.8738383420902986,\n",
              " 0.49601490777476737,\n",
              " 0.029454345118981956,\n",
              " 0.051413762519068006,\n",
              " 0.20650582403426496,\n",
              " 0.0668389650199221,\n",
              " 0.07888768879632856,\n",
              " 0.8966775336391447,\n",
              " 0.49566689081850446,\n",
              " 0.8895971391616966,\n",
              " 0.06609014134034234,\n",
              " 0.018151839472148365,\n",
              " 0.006883366146689514,\n",
              " 0.9108007164639467,\n",
              " 0.15757973839363598,\n",
              " 0.13247657796013051,\n",
              " 0.012758060502465333,\n",
              " 0.31827230640743265,\n",
              " 0.890650035232498,\n",
              " 0.2914404780741598,\n",
              " 0.03647093819627801,\n",
              " 0.8658706900330666,\n",
              " 0.057391808679229994,\n",
              " 0.057475496262721124,\n",
              " 0.007217826957188719,\n",
              " 0.036820096642947144,\n",
              " 0.864157047720984,\n",
              " 0.7644284388700574,\n",
              " 0.11598674102464282,\n",
              " 0.2762130165417239,\n",
              " 0.02753817510553633,\n",
              " 0.04286659660689316,\n",
              " 0.023027050248040613,\n",
              " 0.8566362457723382,\n",
              " 0.05437834742630688,\n",
              " 0.027697398128341436,\n",
              " 0.8810884071975255,\n",
              " 0.10292884414049491,\n",
              " 0.025268350330744715,\n",
              " 0.3038552564330225,\n",
              " 0.33468611756445765,\n",
              " 0.028244734964470324,\n",
              " 0.16879629525234552,\n",
              " 0.14552373413839317,\n",
              " 0.8997551213086814,\n",
              " 0.015349251245182473,\n",
              " 0.6087326349560148,\n",
              " 0.9148725354758251,\n",
              " 0.920726241994025,\n",
              " 0.012539155625216057,\n",
              " 0.1692142089888976,\n",
              " 0.4983499432451316,\n",
              " 0.8511357984800443,\n",
              " 0.8366563845859535,\n",
              " 0.02545899047067429,\n",
              " 0.046127083564158185,\n",
              " 0.47009580041867277,\n",
              " 0.01414254026928442,\n",
              " 0.09848913336949063,\n",
              " 0.8815397945998334,\n",
              " 0.9298516940480914,\n",
              " 0.09919556361767207,\n",
              " 0.013440170041963509,\n",
              " 0.42040403656927827,\n",
              " 0.9324395313915311,\n",
              " 0.17799700756920575,\n",
              " 0.01715494015999682,\n",
              " 0.016645391227952323,\n",
              " 0.4041341897105628,\n",
              " 0.03175361811907996,\n",
              " 0.39393316782718707,\n",
              " 0.02433582989960589,\n",
              " 0.029120550341855716,\n",
              " 0.09912517333909059,\n",
              " 0.08628220953880394,\n",
              " 0.05047109607575036,\n",
              " 0.6877185657157087,\n",
              " 0.35624971791154575,\n",
              " 0.038284199624184036,\n",
              " 0.01682358055892762,\n",
              " 0.16550616283755118,\n",
              " 0.8961072678056278,\n",
              " 0.0700390233846869,\n",
              " 0.9323854594116624,\n",
              " 0.992358223038206,\n",
              " 0.13889830301956554,\n",
              " 0.15981946641586434,\n",
              " 0.8969985538844869,\n",
              " 0.8514862750753898,\n",
              " 0.041212096666745426,\n",
              " 0.1247497470078028,\n",
              " 0.11202057761859176,\n",
              " 0.6686042425420876,\n",
              " 0.055711758831314705,\n",
              " 0.051971565404767946,\n",
              " 0.4153543051159299,\n",
              " 0.8257977499647307,\n",
              " 0.026488857005851775,\n",
              " 0.00936719202183437,\n",
              " 0.0019409703346995391,\n",
              " 0.37128707871932287,\n",
              " 0.218169092700497,\n",
              " 0.7046193569233822,\n",
              " 0.007401273668039662,\n",
              " 0.0944020919152956,\n",
              " 0.8618653328340679,\n",
              " 0.8697095971903475,\n",
              " 0.4393720096627213,\n",
              " 0.04218275769257585,\n",
              " 0.06012348943899468,\n",
              " 0.006556974001497815,\n",
              " 0.03097316009383681,\n",
              " 0.012454040251361476,\n",
              " 0.8937588118209753,\n",
              " 0.023293063876009733,\n",
              " 0.010410967014126884,\n",
              " 0.02474777353999355,\n",
              " 0.0169480527994133,\n",
              " 0.9384718659768514,\n",
              " 0.023320906941917477,\n",
              " 0.06823620420321885,\n",
              " 0.058313189163701894,\n",
              " 0.427901564102925,\n",
              " 0.054021718255599246,\n",
              " 0.6876752620932389,\n",
              " 0.07933302322462629,\n",
              " 0.9154141933106359,\n",
              " 0.06773315739697346,\n",
              " 0.13717780617398095,\n",
              " 0.012413689508309981,\n",
              " 0.03475622919826872,\n",
              " 0.9002694927671102,\n",
              " 0.6343891606890589,\n",
              " 0.2697941032257289,\n",
              " 0.02086176173788139,\n",
              " 0.49564498085958525,\n",
              " 0.9586637338803636,\n",
              " 0.01410450855901,\n",
              " 0.0359481633510094,\n",
              " 0.8994285154095576,\n",
              " 0.2281345107562129,\n",
              " 0.18634426308276145,\n",
              " 0.8028364500387536,\n",
              " 0.01024048825267856,\n",
              " 0.635676034602004,\n",
              " 0.04400568550244938,\n",
              " 0.15134338039469641,\n",
              " 0.05344984366841158,\n",
              " 0.023231017551471504,\n",
              " 0.9086922423966227,\n",
              " 0.15735811355258478,\n",
              " 0.8836417257827123,\n",
              " 0.07137647347025246,\n",
              " 0.9067476165603988,\n",
              " 0.6120831998535173,\n",
              " 0.16270338042526802,\n",
              " 0.05033476666624709,\n",
              " 0.267292801045881,\n",
              " 0.033597809259347994,\n",
              " 0.04227745066734264,\n",
              " 0.9768270041171798,\n",
              " 0.003279875530664703,\n",
              " 0.016839933931343157,\n",
              " 0.555765361373901,\n",
              " 0.8267832552579368,\n",
              " 0.1774094087393369,\n",
              " 0.0412435621375848,\n",
              " 0.33962111697972797,\n",
              " 0.024272020533129244,\n",
              " 0.05304959561993634,\n",
              " 0.5445558373873076,\n",
              " 0.005082113714571408,\n",
              " 0.03864497810687993,\n",
              " 0.9505952973337346,\n",
              " 0.05935726890723904,\n",
              " 0.8447056029277822,\n",
              " 0.8277617920145314,\n",
              " 0.12665988681986473,\n",
              " 0.016771851389663903,\n",
              " 0.12920931852026168,\n",
              " 0.004796296484603903,\n",
              " 0.10609827963035019,\n",
              " 0.07533034968451484,\n",
              " 0.8437833143506527,\n",
              " 0.9264839233719605,\n",
              " 0.956009290317147,\n",
              " 0.857747176276058,\n",
              " 0.9736456509723157,\n",
              " 0.007020198263763153,\n",
              " 0.01996643102464865,\n",
              " 0.028873225631537533,\n",
              " 0.0404688392526025,\n",
              " 0.8160525247484005,\n",
              " 0.9084378224548045,\n",
              " 0.05735283419825861,\n",
              " 0.22366212864525525,\n",
              " 0.14904390839190013,\n",
              " 0.034875336115683206,\n",
              " 0.5101087927361521,\n",
              " 0.723650020946613,\n",
              " 0.9331730350496925,\n",
              " 0.00537381215514214,\n",
              " 0.8283199185064329,\n",
              " 0.02495584759118194,\n",
              " 0.10776154105804163,\n",
              " 0.8695126175762721,\n",
              " 0.3104826703420037,\n",
              " 0.7469866118657146,\n",
              " 0.03973543314696996,\n",
              " 0.021698973730993564,\n",
              " 0.020005410329978164,\n",
              " 0.09803328218618176,\n",
              " 0.9457141692703728,\n",
              " 0.017037411423912834,\n",
              " 0.03814378938817592,\n",
              " 0.9486002202702771,\n",
              " 0.021446714440737043,\n",
              " 0.03378097526922769,\n",
              " 0.8387322346041824,\n",
              " 0.07372791916430765,\n",
              " 0.04313848274773318,\n",
              " 0.8355798640061594,\n",
              " 0.028922275864909707,\n",
              " 0.9057833296040133,\n",
              " 0.8990050249441687,\n",
              " 0.2588914522096217,\n",
              " 0.02982155871491935,\n",
              " 0.05236512253352077,\n",
              " 0.7631361121331064,\n",
              " 0.1046697442865165,\n",
              " 0.9755857203541602,\n",
              " 0.09282683016963618,\n",
              " 0.8634654027754725,\n",
              " 0.04033922701072704,\n",
              " 0.8775685875157136,\n",
              " 0.0058678223462118205,\n",
              " 0.1472854968951776,\n",
              " 0.9323857682549515,\n",
              " 0.6837901353345949,\n",
              " 0.8924768936176265,\n",
              " 0.9621479643705023,\n",
              " 0.15972133682130815,\n",
              " 0.09580583099851206,\n",
              " 0.024256759107947824,\n",
              " 0.6150311737781047,\n",
              " 0.011160871821688621,\n",
              " 0.03519477940677086,\n",
              " 0.3542617548476483,\n",
              " 0.019455079672990535,\n",
              " 0.13001016082598094,\n",
              " 0.08293563821096524,\n",
              " 0.7705799760801004,\n",
              " 0.06825520095692983,\n",
              " 0.8629764198959642,\n",
              " 0.014361809603261713,\n",
              " 0.11019944970258742,\n",
              " 0.029136188093159522,\n",
              " 0.7786740524794468,\n",
              " 0.7498490801320571,\n",
              " 0.35692798067486753,\n",
              " 0.10704860419124954,\n",
              " 0.2633536122266083,\n",
              " 0.47462220089644247,\n",
              " 0.8264076967463477,\n",
              " 0.02830565475294958,\n",
              " 0.025658974682403874,\n",
              " 0.01736271484157769,\n",
              " 0.8729264898561406,\n",
              " 0.13048864625136222,\n",
              " 0.13534536374112574,\n",
              " 0.8565972948949558,\n",
              " 0.3526812049362613,\n",
              " 0.0336320016942403,\n",
              " 0.04296118379090012,\n",
              " 0.021599391273972326,\n",
              " 0.8801425660522679,\n",
              " 0.0544799802622744,\n",
              " 0.11829181018437793,\n",
              " 0.931923702369652,\n",
              " 0.04162470497618823,\n",
              " 0.9032865129175279,\n",
              " 0.09414641134086967,\n",
              " 0.13686081986192547,\n",
              " 0.030813537654484736,\n",
              " 0.026375539471830682,\n",
              " 0.03438706304432069,\n",
              " 0.07889851446547182,\n",
              " 0.911928122790762,\n",
              " 0.01923948653180854,\n",
              " 0.5834798036261973,\n",
              " 0.016854942979715553,\n",
              " 0.8676652067690009,\n",
              " 0.0150443220671381,\n",
              " 0.8494457184734611,\n",
              " 0.02513886354920743,\n",
              " 0.019614458939057746,\n",
              " 0.8328164744332979,\n",
              " 0.04689612788192113,\n",
              " 0.35107693655750566,\n",
              " 0.10582075854036502,\n",
              " 0.0019115571006218344,\n",
              " 0.030619845518918283,\n",
              " 0.05097686817349353,\n",
              " 0.024260997171778396,\n",
              " 0.44866741838361623,\n",
              " 0.9108512440281161,\n",
              " 0.0537104551002021,\n",
              " 0.21231703402809288,\n",
              " 0.18423377825751527,\n",
              " 0.03439285329440536,\n",
              " 0.072980606321747,\n",
              " 0.8181776767681537,\n",
              " 0.33656900184583605,\n",
              " 0.8595169181724268,\n",
              " 0.9261640894999258,\n",
              " 0.0031721654132162973,\n",
              " 0.02182706001640476,\n",
              " 0.013763682532832562,\n",
              " 0.04459528012406472,\n",
              " 0.0255282238073832,\n",
              " 0.311866693470858,\n",
              " 0.008599964572256641,\n",
              " 0.033606114852912566,\n",
              " 0.045559775040830634,\n",
              " 0.32448133996741957,\n",
              " 0.007995333626784843,\n",
              " 0.539786652348748,\n",
              " 0.00720954800774932,\n",
              " 0.05444501281993391,\n",
              " 0.09790664275976,\n",
              " 0.5937940679770021,\n",
              " 0.02274810257113803,\n",
              " 0.07384720593082994,\n",
              " 0.7395737645502802,\n",
              " 0.1949752157925763,\n",
              " 0.00832923259486656,\n",
              " 0.9780469813395778,\n",
              " 0.007792362894178657,\n",
              " 0.04226891411631158,\n",
              " 0.8149188815683938,\n",
              " 0.021961385667914644,\n",
              " 0.28142627865198067,\n",
              " 0.0009551764195648354,\n",
              " 0.012886648249573649,\n",
              " 0.8886004486174907,\n",
              " 0.0579343437729169,\n",
              " 0.46151436849163396,\n",
              " 0.2542703312855543,\n",
              " 0.02559685730522855,\n",
              " 0.0914481763606034,\n",
              " 0.1534535810080439,\n",
              " 0.018187349050086066,\n",
              " 0.04400926946369753,\n",
              " 0.5719477431027653,\n",
              " 0.5006483464624398,\n",
              " 0.05705721140571003,\n",
              " 0.4735309409800039,\n",
              " 0.010727260977086392,\n",
              " 0.03359897249907203,\n",
              " 0.29374166600898727,\n",
              " 0.017044118819718564,\n",
              " 0.9191992590539885,\n",
              " 0.40461517373852485,\n",
              " 0.019706832411109126,\n",
              " 0.03313312947202256,\n",
              " 0.14032362448635138,\n",
              " 0.19443710988645077,\n",
              " 0.24362607521627752,\n",
              " 0.8681492401529295,\n",
              " 0.029479149380527307,\n",
              " 0.1016148640499487,\n",
              " 0.9061152395917204,\n",
              " 0.0585111411653167,\n",
              " 0.016271900928031192,\n",
              " 0.00878048665184612,\n",
              " 0.8123253987415381,\n",
              " 0.8583462715386396,\n",
              " 0.228095503800727,\n",
              " 0.029056077440099786,\n",
              " 0.8416866346006328,\n",
              " 0.021047402306590007,\n",
              " 0.032223115460082,\n",
              " 0.046485950261943944,\n",
              " 0.21162720529707055,\n",
              " 0.8623303562136625,\n",
              " 0.03382492992159394,\n",
              " 0.3910329447854863,\n",
              " 0.04380552704805937,\n",
              " 0.11677425515376334,\n",
              " 0.03360323795728726,\n",
              " 0.8405905726823995,\n",
              " 0.8490463097789772,\n",
              " 0.9015415718568816,\n",
              " 0.7672432254117652,\n",
              " 0.021318037542865813,\n",
              " 0.022819938025646143,\n",
              " 0.08752465466681497,\n",
              " 0.17883256858101007,\n",
              " 0.22081314202461436,\n",
              " 0.1469792601275195,\n",
              " 0.0110022094396818,\n",
              " 0.8977305657857678,\n",
              " 0.8853893101849628,\n",
              " 0.922495818742609,\n",
              " 0.017774257349651972,\n",
              " 0.8895490580345771,\n",
              " 0.8947873805701511,\n",
              " 0.016852018256143464,\n",
              " 0.23859592444840783,\n",
              " 0.37077462378118686,\n",
              " 0.012491059106745908,\n",
              " 0.035563718667048196,\n",
              " 0.783607689860625,\n",
              " 0.06598146939345931,\n",
              " 0.3949744076505769,\n",
              " 0.8416981923141187,\n",
              " 0.008864349672475314,\n",
              " 0.1138046212906615,\n",
              " 0.269789608930587,\n",
              " 0.010951427833450898,\n",
              " 0.825647676335968,\n",
              " 0.8757134841644201,\n",
              " 0.008419056752401538,\n",
              " 0.8409413864307483,\n",
              " 0.9555673429827132,\n",
              " 0.9466861711011764,\n",
              " 0.021903556306013292,\n",
              " 0.10783296875734268,\n",
              " 0.8452439867398225,\n",
              " 0.22548670206346716,\n",
              " 0.02261769330575876,\n",
              " 0.21430380884427486,\n",
              " 0.6048534352603261,\n",
              " 0.16603300521004807,\n",
              " 0.23973482991516476,\n",
              " 0.8672198568136915,\n",
              " 0.052044523541549564,\n",
              " 0.1399942905829469,\n",
              " 0.02566723440295349,\n",
              " 0.03199763741776982,\n",
              " 0.35164109387853415,\n",
              " 0.3683246046056885,\n",
              " 0.08542022743715716,\n",
              " 0.9248936745845404,\n",
              " 0.005520033381833402,\n",
              " 0.0969459129163001,\n",
              " 0.8476920169593293,\n",
              " 0.8610197913876325,\n",
              " 0.4937834697032741,\n",
              " 0.8972377319860738,\n",
              " 0.9304160825423683,\n",
              " 0.011103426031172797,\n",
              " 0.09578317264325514,\n",
              " 0.015215087956962192,\n",
              " 0.025756317068182016,\n",
              " 0.8065704801182038,\n",
              " 0.9130333547217316,\n",
              " 0.06625710465734022,\n",
              " 0.7197142739866871,\n",
              " 0.6030825033509434,\n",
              " 0.02820001897347417,\n",
              " 0.028177893052745064,\n",
              " 0.9566105055375596,\n",
              " 0.053826149322193054,\n",
              " 0.578147620672742,\n",
              " 0.633148593251166,\n",
              " 0.019371431960804594,\n",
              " 0.08373513043037986,\n",
              " 0.03455486468057022,\n",
              " 0.4526000337736419,\n",
              " 0.04536168023790251,\n",
              " 0.05243614847348694,\n",
              " 0.12855435225481804,\n",
              " 0.5171599503117902,\n",
              " 0.008479179920196922,\n",
              " 0.03664028056820406,\n",
              " 0.018352915906173394,\n",
              " 0.021767027518173103,\n",
              " 0.05474444127237516,\n",
              " 0.010589612578620644,\n",
              " 0.017381359586011433,\n",
              " 0.7450622901898258,\n",
              " 0.017411351739805424,\n",
              " 0.21066699847684356,\n",
              " 0.24294972496681982,\n",
              " 0.09876761507676435,\n",
              " 0.0777537908699334,\n",
              " 0.9161510496126697,\n",
              " 0.05735174529430171,\n",
              " 0.046185133715762834,\n",
              " 0.023159286797088927,\n",
              " 0.03997200412600961,\n",
              " 0.2838398890151658,\n",
              " 0.8331199172019421,\n",
              " 0.02810729072654456,\n",
              " 0.014316337287884456,\n",
              " 0.8597157957988621,\n",
              " 0.01992162397472938,\n",
              " 0.8639176925684288,\n",
              " 0.02657907678879427,\n",
              " 0.10376693454946352,\n",
              " 0.04583751967083218,\n",
              " 0.05181882418073468,\n",
              " 0.003813004815019005,\n",
              " 0.07605500660356165,\n",
              " 0.013650896427578368,\n",
              " 0.22253661920483658,\n",
              " 0.60100491372984,\n",
              " 0.7891775792370173,\n",
              " 0.030933297742311825,\n",
              " 0.9757877777330082,\n",
              " 0.6908130427590297,\n",
              " 0.748950512658301,\n",
              " 0.01435520750332264,\n",
              " 0.7536164464863215,\n",
              " 0.011867376779230355,\n",
              " 0.15209992498868077,\n",
              " 0.8986976488884506,\n",
              " 0.249616358958386,\n",
              " 0.020227100608062102,\n",
              " 0.5648852975344317,\n",
              " 0.35140107371497775,\n",
              " 0.5449324219788031,\n",
              " 0.17329932120886765,\n",
              " 0.9258151947861972,\n",
              " 0.007492759818060273,\n",
              " 0.02297572722017801,\n",
              " 0.035685937846022,\n",
              " 0.9392682453143801,\n",
              " 0.1349778350755926,\n",
              " 0.9279309825821125,\n",
              " 0.9505056633743576,\n",
              " 0.009955767408856031,\n",
              " 0.025041675151778145,\n",
              " 0.24027496876744683,\n",
              " 0.02483740080977699,\n",
              " 0.06514684263294022,\n",
              " 0.5275949425579338,\n",
              " 0.4315968496815861,\n",
              " 0.30187897792440854,\n",
              " 0.010240830952000134,\n",
              " 0.4650094872883325,\n",
              " 0.8598994185606482,\n",
              " 0.928001685144808,\n",
              " 0.11985225220415692,\n",
              " 0.05479274165219412,\n",
              " 0.003414623929153375,\n",
              " 0.018919558619061663,\n",
              " 0.5831456181477612,\n",
              " 0.7334972289241439,\n",
              " 0.11689250688186954,\n",
              " 0.7137636311927684,\n",
              " 0.037167899637029836,\n",
              " 0.8132721974265907,\n",
              " 0.13838757017430628,\n",
              " 0.0863833236749997,\n",
              " 0.020998475977358513,\n",
              " 0.021832973983969134,\n",
              " 0.06383347552126442,\n",
              " 0.012046533048432251,\n",
              " 0.0353223267914375,\n",
              " 0.03612814401962373,\n",
              " 0.04818577708490911,\n",
              " 0.7521003095379948,\n",
              " 0.27589972210945096,\n",
              " 0.8450776467342367,\n",
              " 0.42353134777320006,\n",
              " 0.9310836382221916,\n",
              " 0.012971594037692442,\n",
              " 0.04901968347370962,\n",
              " 0.864533601018465,\n",
              " 0.050230223651355405,\n",
              " 0.03133863013370628,\n",
              " 0.8563459894088452,\n",
              " 0.10341601603870004,\n",
              " 0.10777324862292953,\n",
              " 0.8809273304471499,\n",
              " 0.8282856347892665,\n",
              " 0.5959888006843884,\n",
              " 0.8431687006825811,\n",
              " 0.06485474149998917,\n",
              " 0.03455440075936285,\n",
              " 0.03784503705513041,\n",
              " 0.9804649569965797,\n",
              " 0.8643961512796909,\n",
              " 0.007037098017420167,\n",
              " 0.05229497118722213,\n",
              " 0.032097698202075954,\n",
              " 0.37876440300733494,\n",
              " 0.020565890012110958,\n",
              " 0.9194596101548794,\n",
              " 0.08852605899777956,\n",
              " 0.8414197982203618,\n",
              " 0.022463137589386933,\n",
              " 0.1788953643856627,\n",
              " 0.29864087559194596,\n",
              " 0.7481322588513284,\n",
              " 0.12632332219459705,\n",
              " 0.8647018090016548,\n",
              " 0.024147961527022695,\n",
              " 0.06791285454057067,\n",
              " 0.004066261443094912,\n",
              " 0.8999777519326545,\n",
              " 0.03993307663400269,\n",
              " 0.06337055113200574,\n",
              " 0.18621102928509192,\n",
              " 0.828641446364507,\n",
              " 0.9013237991163539,\n",
              " 0.8045798144761483,\n",
              " 0.6747810277947891,\n",
              " 0.0029821911999751297,\n",
              " 0.08061070775335234,\n",
              " 0.03324506441351752,\n",
              " 0.5458630095682441,\n",
              " 0.04847593780369056,\n",
              " 0.010876320835054945,\n",
              " 0.7527173214572621,\n",
              " 0.872277856798823,\n",
              " 0.024566278218971657,\n",
              " 0.7018419185463555,\n",
              " 0.037647202414609664,\n",
              " 0.023279358096277326,\n",
              " 0.027150281263951144,\n",
              " 0.016703050035036655,\n",
              " 0.02195886776919482,\n",
              " 0.022018874908626265,\n",
              " 0.14996616909902852,\n",
              " 0.4749364903037239,\n",
              " 0.012064980635074898,\n",
              " 0.009177374541518584,\n",
              " 0.06288759688816595,\n",
              " 0.3140126354489785,\n",
              " 0.03131291825001726,\n",
              " 0.007250034456654786,\n",
              " 0.8373998708842948,\n",
              " 0.12319614720736015,\n",
              " 0.7930058797773873,\n",
              " 0.05983263635165789,\n",
              " 0.38130218483919287,\n",
              " 0.036731523646596154,\n",
              " 0.29626736907309814,\n",
              " 0.8358601401277783,\n",
              " 0.011835800910494595,\n",
              " 0.19745352151583634,\n",
              " 0.026568000822281182,\n",
              " 0.015581273697391816,\n",
              " 0.8904155370527067,\n",
              " 0.02526171739667515,\n",
              " 0.9111378196374369,\n",
              " 0.0030903694083359804,\n",
              " 0.7907516927385214,\n",
              " 0.06798930935423572]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}