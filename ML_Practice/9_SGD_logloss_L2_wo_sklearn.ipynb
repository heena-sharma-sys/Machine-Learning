{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CopyTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eiDWcM_MC3H"
      },
      "source": [
        "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfe2NTQtLq11"
      },
      "source": [
        "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5DSPCLxqT-"
      },
      "source": [
        "<font color='red'> Importing packages</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Et8BKIxnsp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpSk3WQBx7TQ"
      },
      "source": [
        "<font color='red'>Creating custom dataset</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsMp0oWzx6dv"
      },
      "source": [
        "# please don't change random_state\n",
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
        "# make_classification is used to create custom dataset \n",
        "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8W2fg1cyGdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f55178a-9d18-4712-cbc5-5a2302a840ea"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x99RWCgpqNHw"
      },
      "source": [
        "<font color='red'>Splitting data into train and test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kh4dBfVyJMP"
      },
      "source": [
        "#please don't change random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONY1YiDq7jD"
      },
      "source": [
        "# Standardizing the data.\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(X_train)\n",
        "x_test = scaler.transform(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DR_YMBsyOci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f6564b-a08f-40aa-b8a2-abf485a810a4"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW4OHswfqjHR"
      },
      "source": [
        "# <font color='red' size=5>SGD classifier</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HpvTwDHyQQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f08292-c3f7-4b80-d863-c2cf435e6a50"
      },
      "source": [
        "# alpha : float\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf\n",
        "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYaVyQ2lyXcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9cec0d-da0e-4bb5-ec89-0f723dbd29db"
      },
      "source": [
        "clf.fit(X=x_train, y=y_train) # fitting our model"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.70, NNZs: 15, Bias: -0.501317, T: 37500, Avg. loss: 0.552526\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.752393, T: 75000, Avg. loss: 0.448021\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.26, NNZs: 15, Bias: -0.902742, T: 112500, Avg. loss: 0.415724\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.43, NNZs: 15, Bias: -1.003816, T: 150000, Avg. loss: 0.400895\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.55, NNZs: 15, Bias: -1.076296, T: 187500, Avg. loss: 0.392879\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.65, NNZs: 15, Bias: -1.131077, T: 225000, Avg. loss: 0.388094\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.73, NNZs: 15, Bias: -1.171791, T: 262500, Avg. loss: 0.385077\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.80, NNZs: 15, Bias: -1.203840, T: 300000, Avg. loss: 0.383074\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.86, NNZs: 15, Bias: -1.229563, T: 337500, Avg. loss: 0.381703\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.90, NNZs: 15, Bias: -1.251245, T: 375000, Avg. loss: 0.380763\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.94, NNZs: 15, Bias: -1.269044, T: 412500, Avg. loss: 0.380084\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1.98, NNZs: 15, Bias: -1.282485, T: 450000, Avg. loss: 0.379607\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2.01, NNZs: 15, Bias: -1.294386, T: 487500, Avg. loss: 0.379251\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2.03, NNZs: 15, Bias: -1.305805, T: 525000, Avg. loss: 0.378992\n",
            "Total training time: 0.15 seconds.\n",
            "Convergence after 14 epochs took 0.15 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfkVI6GyaRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d2b2fb-91a6-4725-a714-110ba1e6dab4"
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_\n",
        "#clf.coef_ will return the weights\n",
        "#clf.coef_.shape will return the shape of weights\n",
        "#clf.intercept_ will return the intercept term"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.89007184,  0.63162363, -0.07594145,  0.63107107, -0.38434375,\n",
              "          0.93235243, -0.89573521, -0.07340522,  0.40591417,  0.4199991 ,\n",
              "          0.24722143,  0.05046199, -0.08877987,  0.54081652,  0.06643888]]),\n",
              " (1, 15),\n",
              " array([-1.30580538]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-CcGTKgsMrY"
      },
      "source": [
        "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_8bdzitDlM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.  We will be giving you some functions, please write code in that functions only.\n",
        "\n",
        "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2Y3-FQuJ3z"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
        "\n",
        "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
        "\n",
        " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
        "- for each epoch:\n",
        "\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
        "\n",
        "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
        "\n",
        "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
        "\n",
        "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
        "\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
        "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
        "\n",
        "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
        "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
        "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_HgjgS_wKu"
      },
      "source": [
        "<font color='blue'>Initialize weights </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecwYV9fsKZ9"
      },
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    #initialize the weights to zeros array of (1,dim) dimensions\n",
        "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
        "    #initialize bias to zero\n",
        "    w = np.zeros_like(dim)\n",
        "    b = 0\n",
        "    return w,b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7I6uWBRsKc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704a7519-3f70-4b15-cdb4-86180f167d1a"
      },
      "source": [
        "dim = x_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "print('w =',(w), w.size)\n",
        "print('b =',str(b))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 15\n",
            "b = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MI5SAjP9ofN"
      },
      "source": [
        "<font color='cyan'>Grader function - 1 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1llH429wG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ad14ad-1ba2-4605-b0cb-786f162b9ed5"
      },
      "source": [
        "dim=x_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN83oMWy_5rv"
      },
      "source": [
        "<font color='blue'>Compute sigmoid </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPv4NJuxABgs"
      },
      "source": [
        "$sigmoid(z)= 1/(1+exp(-z))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAfmQF47_Sd6"
      },
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    sig = 1/(1 + np.exp(-z))\n",
        "    return sig"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrGDwg3Ae4m"
      },
      "source": [
        "<font color='cyan'>Grader function - 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_JASp_NAfK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939701fd-634c-4b6a-a82d-4d4ecb337c88"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  val=sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "grader_sigmoid(2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7JXbcrBOFF"
      },
      "source": [
        "<font color='blue'> Compute loss </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfEiS22zBVYy"
      },
      "source": [
        "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaFDgsp3sKi6"
      },
      "source": [
        "import math\n",
        "\n",
        "def logloss(y_true, y_pred):\n",
        "    '''In this function, we will compute log loss '''\n",
        "    '''\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    logloss = -1 * np.mean( y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "    '''\n",
        "    logloss=0\n",
        "    for i in range(len(y_true)):\n",
        "        logloss += ((y_true[i]*math.log10(y_pred[i]))+((1-y_true[i])*math.log10(1-y_pred[i])))\n",
        "    logloss = -1*(1/len(y_true))*logloss\n",
        "    return logloss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1BTXVSClBt"
      },
      "source": [
        "<font color='cyan'>Grader function - 3 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzttjvBFCuQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6aa49f9-1cff-4735-9c0d-bafc65dd731a"
      },
      "source": [
        "def grader_logloss(true1,pred):\n",
        "  loss=logloss(true1,pred)\n",
        "  assert(loss==0.07644900402910389)\n",
        "  return True\n",
        "true1=[1,1,0,1,0]\n",
        "pred=[0.9,0.8,0.1,0.8,0.2]\n",
        "grader_logloss(true1,pred)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQabIadLCBAB"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to  'w' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMxiYKaCQgd"
      },
      "source": [
        "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVikyuFsKo5"
      },
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w)/N)\n",
        "    \n",
        "    return dw"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFLNqL_GER9"
      },
      "source": [
        "<font color='cyan'>Grader function - 4 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI3xD8ctGEnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4122c148-637b-4126-c006-5f29eeb24903"
      },
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "  assert(np.sum(grad_dw)==2.613689585)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8g84_GI62n"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to 'b' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHvTYZzZJJ_N"
      },
      "source": [
        "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUf2ft4EZp8"
      },
      "source": [
        " def gradient_db(x,y,w,b):\n",
        "    '''In this function, we will compute gradient w.r.to b '''\n",
        "    db = y-sigmoid(np.dot(w.T,x)+b)\n",
        "    return db"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcBzufVG6qk"
      },
      "source": [
        "<font color='cyan'>Grader function - 5 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfFDKmscG5qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2203f1a7-58ce-4935-fd38-661c7fdfecaf"
      },
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCK0jY_EOvyU"
      },
      "source": [
        "<font color='blue'> Implementing logistic regression</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRkTyI_CUGhd"
      },
      "source": [
        "def train(X_train, y_train, X_test, y_test, epochs, alpha, eta0):\n",
        "    w,b = initialize_weights(X_train[0])\n",
        "    N=len(X_train)\n",
        "    log_loss_train = []\n",
        "    log_loss_test = []\n",
        "\n",
        "    for i in range(0, epochs):\n",
        "        for j in range(N):\n",
        "            grad_dw = gradient_dw(X_train[j], y_train[j], w, b, alpha, N)\n",
        "            grad_db = gradient_db(X_train[j], y_train[j], w, b)\n",
        "            w       = np.array(w) + (eta0*(np.array(grad_dw)))\n",
        "            b       = b + (eta0*(grad_db))   \n",
        "        \n",
        "        # predict the output of x_train[for all data points in X_train] using w,b\n",
        "        predict_train = []\n",
        "        for m in range(len(y_train)):\n",
        "            z = np.dot(w, X_train[m])+b\n",
        "            predict_train.append(sigmoid(z)) \n",
        "        \n",
        "        # compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the train loss values in a list\n",
        "        train_loss = logloss(y_train, predict_train)\n",
        "        \n",
        "        # predict the output of x_test[for all data points in X_test] using w,b\n",
        "        predict_test = []\n",
        "        for m in range(len(y_test)):\n",
        "            z = np.dot(w, X_test[m])+b\n",
        "            predict_test.append(sigmoid(z)) \n",
        "\n",
        "        # compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the test loss values in a list\n",
        "        test_loss = logloss(y_test, predict_test)\n",
        "\n",
        "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
        "        if log_loss_train and train_loss > log_loss_train[-1]: # and log_loss_test and test_loss > log_loss_test[-1]:\n",
        "          return w, b, log_loss_train, log_loss_test\n",
        "        \n",
        "        log_loss_train.append(train_loss)\n",
        "        log_loss_test.append(test_loss)\n",
        "          \n",
        "    return w, b, log_loss_train, log_loss_test"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUquz7LFEZ6E"
      },
      "source": [
        "alpha  = 0.0001\n",
        "eta0   = 0.0001\n",
        "epochs = 50\n",
        "w, b, log_loss_train, log_loss_test = train(x_train, y_train, x_test, y_test, epochs, alpha, eta0)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bbj8wkCMfcd",
        "outputId": "6f66d0e0-c2cf-4db0-f86a-144d75b7be6f"
      },
      "source": [
        "print (\"weight vector: \", w)\n",
        "print (\"Intercept: \", b) \n",
        "print (\"log loss train\", log_loss_train)\n",
        "print (\"log loss test\", log_loss_test)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight vector:  [-0.9712546   0.6951594  -0.10648865  0.68159052 -0.44472549  1.00799631\n",
            " -0.94341139 -0.07316671  0.44633494  0.47814801  0.27402297  0.06013629\n",
            " -0.09610535  0.57042941  0.06404647]\n",
            "Intercept:  -1.369139915843557\n",
            "log loss train [0.20729781784140838, 0.18556210141426163, 0.17659652085620509, 0.17201289496451905, 0.16938000886115878, 0.16775336575455, 0.1666977629761566, 0.1659883750043287, 0.16549918227604976, 0.16515513945496194, 0.16490944296095902, 0.16473183113949116, 0.1646021696464541, 0.16450674989278702, 0.16443606134127778, 0.1643834031232729, 0.16434399300380859, 0.16431438119164662, 0.1642920564949399, 0.16427517690890994, 0.16426238246016317, 0.1642526634578411, 0.16424526668148529, 0.16423962791724517, 0.1642353230239534, 0.16423203217373442, 0.16422951354993254, 0.16422758389129435, 0.16422610402903806, 0.16422496808893378, 0.1642240953990856, 0.16422342440533785, 0.16422290808287712, 0.1642225104673144, 0.1642222040262155, 0.1642219676635313, 0.16422178520193, 0.16422164422678262, 0.16422153520444804, 0.16422145080897918, 0.1642213854074215, 0.16422133466598673, 0.1642212952484606, 0.16422126458506267, 0.1642212406951902, 0.1642212220514016, 0.16422120747495308, 0.16422119605559377, 0.16422118708987574, 0.16422118003373862]\n",
            "log loss test [0.20722219781181883, 0.1856525943467828, 0.17682567720849302, 0.17235324848189565, 0.1698100984080047, 0.16825663498220056, 0.1672612889069227, 0.16660192986644845, 0.16615457121757737, 0.16584572669386236, 0.16562980540333389, 0.16547750036682654, 0.16536943679761335, 0.16529251625482547, 0.16523772268630538, 0.1651987592688669, 0.165171176424903, 0.16515180015954947, 0.16513834939454072, 0.16512917523384424, 0.1651230806039871, 0.1651191938733821, 0.1651168793161568, 0.16511567308220967, 0.16511523704234513, 0.16511532529485734, 0.16511575972396544, 0.16511641208166614, 0.16511719080338336, 0.16511803127917835, 0.16511888866014965, 0.1651197325326434, 0.1651205429733953, 0.16512130762843552, 0.16512201955262923, 0.1651226756151328, 0.16512327532633586, 0.1651238199786852, 0.1651243120212548, 0.16512475460810555, 0.16512515127566163, 0.16512550571555762, 0.16512582161788045, 0.16512610256603702, 0.16512635196923017, 0.16512657302209208, 0.16512676868368145, 0.16512694167010694, 0.16512709445649618, 0.16512722928519263]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Zf_wPARlwY"
      },
      "source": [
        "<font color='red'>Goal of assignment</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3eF_VSPSH2z"
      },
      "source": [
        "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230YbSgNSUrQ"
      },
      "source": [
        "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
        "\n",
        "* epoch number on X-axis\n",
        "* loss on Y-axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "y-NWv32UEHJz",
        "outputId": "281b5503-cfb9-4e26-9257-2c7ec1c56747"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.title(\"Epocs number vs Train/Test log-loss\")\n",
        "x = [x for x in range(len(log_loss_train))]\n",
        "plt.xlabel(\"Epoch Nuumber\")\n",
        "plt.ylabel(\"Train/Test log loss\")\n",
        "plt.plot(x, log_loss_train, label='Train log loss')\n",
        "plt.plot(x, log_loss_test, label='Test log loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUN8puFoEZtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bcb4d9f-f2c8-4cb3-e13e-d0f982348d1e"
      },
      "source": [
        "def pred(w, b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)\n",
        "print(1-np.sum(y_train - pred(w,b,x_train))/len(x_train))\n",
        "print(1-np.sum(y_test  - pred(w,b,x_test))/len(x_test))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9518666666666666\n",
            "0.94936\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}