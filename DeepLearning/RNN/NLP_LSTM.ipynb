{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khcKIAGrh3-j"
   },
   "source": [
    "<center><u><h1>Long Short-Term Memory_LSTM</center></u></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9OwuQvAh30Y"
   },
   "source": [
    "LSTM[Long short term memory] is an improvement over Recurrent Neural Network to address RNN’s failure to learn in the presence of past observations greater than 5–10 discrete time steps between relevant input events and target signals (vanishing/exploding gradient issue). LSTM does so by introducing a memory unit called “cell state”. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuTVRfDXkGV_"
   },
   "source": [
    "![](https://miro.medium.com/max/1400/1*n-IgHZM5baBUjq0T7RYDBw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KUKGFc7ie1x"
   },
   "source": [
    "LSTM networks are an extension of recurrent neural networks mainly introduced to handle situations where RNNs fail. Talking about RNN, it is a network that works on the present input by taking into consideration the previous output and storing in its memory for a short period of time. Out of its various applications, the most popular ones are in the fields of speech processing, non-Markovian control, and music composition. \n",
    "###Drawbacks to RNNs. \n",
    "First, it fails to store information for a longer period of time. At times, a reference to certain information stored quite a long time ago is required to predict the current output. But RNNs are absolutely incapable of handling such “long-term dependencies”.\n",
    "Second, there is no finer control over which part of the context needs to be carried forward and how much of the past needs to be ‘forgotten’.\n",
    "Other issues with RNNs are exploding and vanishing gradients which occur during the training process of a network through backtracking. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Long Short-Term Memory (LSTM)was brought into the picture. It has been so designed that the vanishing gradient problem is almost completely removed, while the training model is left unaltered. Long time lags in certain problems are bridged using LSTMs where they also handle noise, distributed representations, and continuous values. With LSTMs, there is no need to keep a finite number of states from beforehand as required in the hidden Markov model. LSTMs provide us with a large range of parameters such as learning rates, and input and output biases. Hence, no need for fine adjustments. The complexity to update each weight is reduced to O(1) with LSTMs, similar to that of Back Propagation Through Time (BPTT), which is an advantage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vI4qJ4Pyj3lI"
   },
   "source": [
    "#### Exploding and Vanishing Gradients problems: \n",
    "\n",
    "During the training process of a network, the main goal is to minimize loss observed in the output when training data is sent through it. We calculate the gradient, that is, loss with respect to a particular set of weights, adjust the weights accordingly and repeat this process until we get an optimal set of weights for which loss is minimum. This is the concept of backtracking. \n",
    "<br>\n",
    "Sometimes, it so happens that the gradient is almost negligible. It must be noted that the gradient of a layer depends on certain components in the successive layers. If some of these components are small (less than 1), the result obtained, which is the gradient, will be even smaller. This is known as the scaling effect. <br>\n",
    "When this gradient is multiplied with the learning rate which is in itself a small value ranging between 0.1-0.001, it results in a smaller value. As a consequence, the alteration in weights is quite small, producing almost the same output as before. <br>\n",
    "Similarly, if the gradients are quite large in value due to the large values of components, the weights get updated to a value beyond the optimal value. This is known as the problem of exploding gradients. To avoid this scaling effect, the neural network unit was re-built in such a way that the scaling factor was fixed to one. The cell was then enriched by several gating units and was called LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOMW0y5_kp4L"
   },
   "source": [
    "An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM’s cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r2okEHkkfM8"
   },
   "source": [
    "![](https://miro.medium.com/max/700/1*0f8r3Vd-i4ueYND1CUrhMA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEdl0OxSlNNu"
   },
   "source": [
    "Forget layer: This layer filters or removes info/memory from previous cell state based on current input and previous hidden state. This is done via a sigmoid activation function. This function results only 0 and 1 for inputs. Once it is multiplied to something either it will drop that(multiplication with zero) results in zero or completely pass through(anything multiplied by 1 is same)\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*GjehOa513_BgpDDP6Vkw2Q.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJP63ZI5mGkC"
   },
   "source": [
    "Input Layer: This has again a forget logic, which removes any unwanted information from current input. We also have a modulator which keeps the values in between -1 and 1. This is achieved using a tanh activation function.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*TTmYy7Sy8uUXxUXfzmoKbA.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpXoOQeymr6g"
   },
   "source": [
    "Cell State<br>\n",
    "Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant.\n",
    "![](https://miro.medium.com/max/2400/1*S0rXIeO_VoUVOyrYHckUWg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9EAJen8m3Ny"
   },
   "source": [
    "Output Layer: <br>\n",
    "This layer takes current input and current cell state and then outputs the hidden state and cell output. Again we use scaling (tanh) for cell state to keep values in range -1 to 1.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*VOXRGhOShoWWks6ouoDN3Q.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DRoLAlO2-Ym"
   },
   "source": [
    "Let's begin by importing the required libraries.<br>\n",
    "1. We’ll need TensorFlow so we import it as tf.<br>\n",
    "2. From the TensorFlow Keras Datasets, we import the imdb one.<br>\n",
    "3. We’ll need word embeddings i.e Embedding, Dense and LSTM layers.<br>\n",
    "4. Our loss function will be binary cross entropy.<br>\n",
    "5. As we’ll stack all layers on top of each other with model.add, we need Sequential for constructing our model.<br>\n",
    "For optimization we use an extension of classic gradient descent called Adam.<br>\n",
    "6. Finally, we need to import pad_sequences. We’re going to use the IMDB dataset which has sequences of reviews. While we’ll specify a maximum length, this can mean that shorter sequences are present as well; these are not cutoff and therefore have different sizes than our desired one (i.e. the maximum length).<br> \n",
    "7. We’ll have to pad them with zeroes in order to make them of equal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jgTWzhuynx7n"
   },
   "outputs": [],
   "source": [
    "#importing above mentioned libraries.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwWyt6V-4NCC"
   },
   "source": [
    "The next step is specifying the model configuration.<br>\n",
    "You can easily see how your model is configured, without having to take a look through all the aspects.<br>\n",
    "We can see that our model will be trained with a batch size of 128, using binary crossentropy loss and Adam optimization, and only for five epochs (we only have to show you that it works).<br>\n",
    "20% of our training data will be used for validation purposes, and the output will be verbose, with verbosity mode set to 1 out of 0, 1 and 2. Our learned word embedding will have 15 hidden dimensions and each sequence passed through the model is 300 characters at max. Our vocabulary will contain 5000 words at max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "IudlF46RX3aB"
   },
   "outputs": [],
   "source": [
    "# Model configuration of metrics is accuaracy\n",
    "model_metrics=['accuracy']\n",
    "\n",
    "#batch size is 128\n",
    "batch_size=128\n",
    "\n",
    "#embedding hidden dimensions=15[embedding_output_dims]\n",
    "embedding_output_dims=15\n",
    "\n",
    "#loss funciton is BinaryCrossentropy\n",
    "loss_function= BinaryCrossentropy()\n",
    "\n",
    "#max len of sentence is to be 300[max_sequence_length]\n",
    "max_sequence_length=300\n",
    "\n",
    "#Our vocabulary will contain 5000 words at max.[num_distinct_words]\n",
    "num_distinct_words = 5000\n",
    "\n",
    "#nos of epochs=5\n",
    "nos_of_epochs=5\n",
    "\n",
    "#optimizer is adam\n",
    "optimizer=Adam()\n",
    "\n",
    "#20% is used for validation split\n",
    "validation_split=0.20\n",
    "\n",
    "# keep verbosity is 1\n",
    "verbosity_mode=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5_3cXL471kB"
   },
   "source": [
    "You might now also want to disable Eager Execution in TensorFlow. While it doesn’t work for all, some people report that the training process speeds up after using it. However, it’s not necessary to do so – simply test how it behaves on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "FXamZduaX-Xu"
   },
   "outputs": [],
   "source": [
    "# Disable eager execution\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDJ9iF6h8DHZ"
   },
   "source": [
    "Loading and preparing the data:\n",
    "we can load and prepare the data.<br>\n",
    "Keras comes with a standard set of datasets, of which the IMDB dataset can be used for sentiment analysis.<br>\n",
    " we can use imdb.load_data(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7882,
     "status": "ok",
     "timestamp": 1640700600499,
     "user": {
      "displayName": "Harsh Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64",
      "userId": "17024393577710317653"
     },
     "user_tz": -330
    },
    "id": "uNniga81YBb8",
    "outputId": "6735dca4-6bb3-400c-a788-c87949a051ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset by using (x_train, y_train), (x_test, y_test) for num_distinct_words i.e 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)\n",
    "#shape of X train and X test\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDv5g6Vn82YF"
   },
   "source": [
    "Once the data has been loaded, we apply pad_sequences. This ensures that sentences shorter than the maximum sentence length are brought to equal length by applying padding with, in this case, zeroes, because that often corresponds with the padding character.\n",
    "Refer:https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "7VvTRkXdYDs9"
   },
   "outputs": [],
   "source": [
    "# Pad all sequences with pad_sequence for x_train and x_test with max sequence \n",
    "#length for value=0.0 \n",
    "\n",
    "x_train_p = pad_sequences(x_train, maxlen=max_sequence_length, value=0.0)# 0.0 because it corresponds with <PAD>\n",
    "x_test_p = pad_sequences(x_test, maxlen=max_sequence_length, value=0.0)# 0.0 because it corresponds with <PAD>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPp_jeKU9Zc6"
   },
   "source": [
    "We can then define the Keras model.\n",
    "we can initialize the model variable with Sequential().<br>\n",
    "The first layer is an Embedding layer, which learns a word embedding that in our case has a dimensionality of 15. <br>\n",
    "This is followed by an LSTM layer providing the recurrent segment, and a Dense layer that has one output through Sigmoid a number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "GjbHyGrRYIcX"
   },
   "outputs": [],
   "source": [
    "# Define the Keras model\n",
    "#intialize with sequential()\n",
    "model = Sequential()\n",
    "\n",
    "#intialize first layer for embedding with num_distinct_words, embedding_output_dims and max sequence length\n",
    "model.add(Embedding(num_distinct_words, embedding_output_dims, input_length=max_sequence_length))\n",
    "\n",
    "#intialize another layer with LSTM for 10 \n",
    "model.add(LSTM(10))\n",
    "\n",
    "#adding dense layer with 1 output and having activation function of sigmoid\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcCV5hON-cw2"
   },
   "source": [
    "The model can then be compiled. We do so by specifying the optimizer, the loss function, and the  metrics that we had specified before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "iZd-fppwYUBz"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=model_metrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej-gJw9S-oUp"
   },
   "source": [
    "This is also a good place to generate a summary of what the model looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1640700684345,
     "user": {
      "displayName": "Harsh Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64",
      "userId": "17024393577710317653"
     },
     "user_tz": -330
    },
    "id": "ibTx_iahYXhS",
    "outputId": "19cac6f2-8c9a-4aa1-baaa-6216fefdb062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 300, 15)           75000     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 10)                1040      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,051\n",
      "Trainable params: 76,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Give a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqqUKDVW-rFP"
   },
   "source": [
    "Training the Keras model,we can instruct TensorFlow to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123902,
     "status": "ok",
     "timestamp": 1640700858941,
     "user": {
      "displayName": "Harsh Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64",
      "userId": "17024393577710317653"
     },
     "user_tz": -330
    },
    "id": "OXOaTgJUYaPq",
    "outputId": "f1c1e5a3-ed5e-4312-fd6e-792cb0106adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 11s 537us/sample - loss: 0.5930 - accuracy: 0.7158 - val_loss: 0.4628 - val_accuracy: 0.8242\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 10s 489us/sample - loss: 0.3966 - accuracy: 0.8519 - val_loss: 0.4401 - val_accuracy: 0.7942\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 10s 501us/sample - loss: 0.3023 - accuracy: 0.8867 - val_loss: 0.3297 - val_accuracy: 0.8658\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 10s 476us/sample - loss: 0.2520 - accuracy: 0.9068 - val_loss: 0.3237 - val_accuracy: 0.8674\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 10s 501us/sample - loss: 0.2239 - accuracy: 0.9204 - val_loss: 0.3177 - val_accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "# Train the model for padded inputs, y train, batch size , epochs ,verbose and validation spilt.\n",
    "hist = model.fit(x_train_p, y_train, batch_size=batch_size, epochs=nos_of_epochs,\n",
    "                 verbose=verbosity_mode, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW2cMJuE--Fc"
   },
   "source": [
    "The (input, output) pairs passed to the model are the padded inputs and their corresponding class labels. Training happens with the batch size, number of epochs, verbosity mode and validation split that were also defined in the configuration section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ywit53PC_Bdy"
   },
   "source": [
    "Evaluating the Keras model\n",
    "We cannot evaluate the model on the same dataset that was used for training it. We fortunately have testing data available through the train/test split performed in the load_data(...) section, and can use built-in evaluation facilities to evaluate the model. We then print the test results on screen. Evaluate the model using the evaluate method by passing the independent test data and dependent test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "-mJUK0fwYdFR"
   },
   "outputs": [],
   "source": [
    "# Test the model after training\n",
    "test_res = model.evaluate(x_test_p, y_test, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Sy85lkY8MaM"
   },
   "source": [
    "Now print the Test results i.e. the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1640700916060,
     "user": {
      "displayName": "Harsh Shah",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsjDxVj9TyGHzzJHhMwnDDoEO86oODWG-P-xWZJA=s64",
      "userId": "17024393577710317653"
     },
     "user_tz": -330
    },
    "id": "GdgaRhj-ZK0R",
    "outputId": "afab2062-5434-48d0-88a9-4d6427bbd69c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results - Loss: 0.3285440972137451 - Accuracy: 86.66800260543823\n"
     ]
    }
   ],
   "source": [
    "# print the loss and accuracy\n",
    "print('Test results - Loss:',test_res[0],'- Accuracy:', test_res[1] * 100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPu6vV0Gkd9d0s3IXOGjnBP",
   "collapsed_sections": [],
   "name": "NLP_CloudyMl_Assignment-7_wothoutCode.ipynb",
   "provenance": [
    {
     "file_id": "1YVaiOIsvOMhqYj5DosfmMobDKujaXdrN",
     "timestamp": 1641021920025
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
